{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "# from transformers import DistilBertTokenizer, DistilBertModel,DistilBertForSequenceClassification\n",
    "import torch\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining a custom dataset which returns id, is_fake, mask, and text for each item in the train set and returns id, mask, and text for each item in the test set.\n",
    "### The tokenizer to use was bert_base_uncased and each of the sentences is truncated to 512 if its length exceeds 512 and is padded if its length is below 512 since 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureDataset(Dataset):\n",
    "    def __init__(self, file_name, is_train):\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        file_out = pd.read_csv(file_name)\n",
    "        body_text = file_out.loc[:, 'text']\n",
    "        \n",
    "        \n",
    "        if is_train:\n",
    "            is_fake = file_out.loc[:, 'is_fake'].values\n",
    "            self.is_fake = torch.tensor(is_fake).float()\n",
    "            \n",
    "        self.ID = file_out.loc[:, 'id'].values\n",
    "\n",
    "#         tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "        \n",
    "        tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:511], body_text))\n",
    "        tokens_ids = list(map(tokenizer.convert_tokens_to_ids, tokens))\n",
    "        tokens_ids = pad_sequences(tokens_ids, maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
    "        \n",
    "        masks = [[float(i > 0) for i in ii] for ii in tokens_ids]\n",
    "        self.masks = torch.tensor(masks)\n",
    "\n",
    "\n",
    "        self.body_text = torch.tensor(tokens_ids)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.body_text)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.is_train:\n",
    "            return self.body_text[idx], self.masks[idx], self.is_fake[idx], self.ID[idx]\n",
    "        else:\n",
    "            return self.body_text[idx], self.masks[idx], self.ID[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FeatureDataset('train.csv', True)\n",
    "test_set = FeatureDataset('test.csv', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model is bert base uncased with a classifier on top of it with sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBinaryClassifier(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(BertBinaryClassifier, self).__init__()\n",
    "        \n",
    "#         self.bert = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, tokens, masks=None):\n",
    "        _, pooled_output = self.bert(tokens, output_all_encoded_layers=False)\n",
    "#         pooled_output = self.bert(tokens)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        proba = self.sigmoid(linear_output)\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the train data is splitted to train(80%) and valid (20%) and the batch size was set to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "CUDA_VISIBLE_DEVICES=2\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "# train_dataloader = torch.utils.data.DataLoader(train_set, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "\n",
    "dataset_size = len(train_set)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "valid_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n",
    "print(len(train_dataloader))\n",
    "print(len(validation_loader))\n",
    "print(len(test_dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the model was  trained for 10 epochs and the best model according to accuracy on the validation set was saved. The loss is a binary cross entropy and the optimizer is Adam with learning rate =3e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "bert_clf = BertBinaryClassifier()\n",
    "bert_clf = bert_clf.cuda()\n",
    "optimizer = torch.optim.Adam(bert_clf.parameters(), lr=3e-6)\n",
    "accuracy= 0\n",
    "for epoch_num in range(EPOCHS):\n",
    "    bert_clf.train()\n",
    "    train_loss = 0\n",
    "    for step_num, batch_data in enumerate(train_dataloader):\n",
    "        token_ids, masks, labels, IDs = tuple(t for t in batch_data)\n",
    "        token_ids , masks, labels, IDs = token_ids.cuda(), masks.cuda(), labels.cuda(), IDs.cuda()\n",
    "        probas = bert_clf(token_ids, masks)\n",
    "        loss_func = nn.BCELoss()\n",
    "        batch_loss = loss_func(probas, labels)\n",
    "        train_loss += batch_loss.item()\n",
    "        bert_clf.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        print('Epoch: ', epoch_num + 1)\n",
    "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_set) / batch_size, train_loss / (step_num + 1)))\n",
    "    #save the model only if the accuracy on the test set was improved\n",
    "    bert_clf.eval()\n",
    "    predictions = []\n",
    "    for step_num, batch_data in enumerate(validation_loader):\n",
    "        token_ids, masks, true_label, IDs = tuple(t for t in batch_data)\n",
    "        token_ids , masks, labels, IDs = token_ids.cuda(), masks.cuda(), labels.cuda(), IDs.cuda()\n",
    "        probs = bert_clf(token_ids, masks)\n",
    "        probs = probs.detach().cpu().numpy()[0][0]\n",
    "        if probs < 0.5:\n",
    "            pred_label = 0\n",
    "        else:\n",
    "            pred_label =1\n",
    "        predictions.append(pred_label==true_label.cpu().numpy())\n",
    "    if sum(predictions)/len(predictions)>accuracy:\n",
    "        #update accuracy and save the model\n",
    "        accuracy = sum(predictions)/len(predictions)\n",
    "        print('accuracy was improved')\n",
    "        print('accuracy= {}'.format(accuracy))\n",
    "        torch.save(bert_clf.state_dict(), 'bert_clf.pt')\n",
    "    else:\n",
    "        print('accuracy was not improved')\n",
    "# torch.save(bert_clf.state_dict(), 'bert_clf.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_clf = BertBinaryClassifier()\n",
    "bert_clf.load_state_dict(torch.load('bert_clf.pt'))\n",
    "bert_clf.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bert_clf.eval()\n",
    "pred = pd.DataFrame({'id':np.zeros(len(test_set)), 'is_fake':np.zeros(len(test_set))})\n",
    "for step_num, batch_data in enumerate(test_dataloader):\n",
    "        token_ids, masks, IDs = tuple(t for t in batch_data)\n",
    "        IDs = IDs.numpy()[0]\n",
    "        token_ids , masks = token_ids.cuda(), masks.cuda()\n",
    "        probs = bert_clf(token_ids, masks)\n",
    "        probs = probs.detach().cpu().numpy()[0][0]\n",
    "        pred.loc[step_num, 'id'] = IDs\n",
    "        if probs < 0.5:\n",
    "             pred.loc[step_num, 'is_fake'] = 0\n",
    "        else:\n",
    "            pred.loc[step_num, 'is_fake'] = 1\n",
    "        print(step_num)\n",
    "        \n",
    "pred['id']= pred['id'].astype('int32')\n",
    "pred['is_fake']= pred['is_fake'].astype('int32')\n",
    "pred.to_csv('first_prediction.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
